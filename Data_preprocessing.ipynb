{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "4dece573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import operator\n",
    "import matplotlib as plt\n",
    "from pylab import *\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import notebook\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a21875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"D:\\\\Music\\\\Sample Packs\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e473dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\music\\sample packs\\1. delifb chill future bass & trap sounds\\kicks\\kick 9.wav\n"
     ]
    }
   ],
   "source": [
    "paths = [path.lower() for path in glob(f'{data_dir}/**/*.wav', recursive=True)]\n",
    "len(paths)\n",
    "print(paths[123])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be27eb6",
   "metadata": {},
   "source": [
    "### 24 Categories, 17 Genres, 46 Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "ccebb170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9379\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "category = [\"drum\", \"kick\", \"snare\", \"foley\", \"percussion\", \"clap\", \"cymbal\", \"crash\", \"tom\", \"rim\", \"bell\", \"tambourine\",\n",
    "            \"hat\", \"closed_hat\", \"open_hat\", \"ride\", \"depot\", \"snap\", \"shake\", \"reverse_crash\", \"impact\"]\n",
    "genres = [\"future_bass\", \"dubstep\", \"hitech\", \"trap\", \"hardcore\", \"riddim\", \"uk_garage\",\"acoustic\", \"bass_house\", \"pop\",\n",
    "          \"house\", \"future_bounce\", \"future_house\", \"tropical_house\", \"edm\", \"rnb\", \"reggae\", \"hip_hop\", \"hardstyle\", \"orchestral\", \"cinematic\"]\n",
    "style = [\"kshmr\", \"future\", \"acoustic\", \"spicy\", \"tropical\", \"high\", \"low\", \"electro\", \"808\", \"melodic\", \"weird\", \"punch\", \"kawaii\",\n",
    "         \"crunch\", \"epic\", \"wet\", \"lofi\", \"deep\", \"fat\", \"tight\", \"vintage\", \"top\", \"gated\", \"closet\", \"pan\", \"heavy\", \"light\" , \"scratch\", \"roll\", \"noise\",\n",
    "         \"hard\", \"soft\", \"stomp\", \"short\", \"thick\", \"anime\", \"bright\", \"glitch\", \"beatbox\", \"dirty\", \"distort\", \"droplet\", \"reverse\", \"rattle\", \n",
    "         \"djembe\", \"chime\", \"stadium\"]\n",
    "\n",
    "delete = [\"loop\", \"vox\", \"guitar\", \"buildup\", \"fill\", \"construction\", \"riser\", \"impact\", \"brass\"]\n",
    "\n",
    "for path in paths :\n",
    "    if any(keyword in path for keyword in category) and not any(keyword in path for keyword in delete) :\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "23624ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_dir</th>\n",
       "      <th>category</th>\n",
       "      <th>genre</th>\n",
       "      <th>style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [data_dir, category, genre, style]\n",
       "Index: []"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = {}\n",
    "df = pd.DataFrame(columns=[\"data_dir\", \"category\", \"genre\", \"style\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8e501883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14be444c268747a699751524e8dcc92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "keyword processing...:   0%|          | 0/30769 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = notebook.tqdm(paths, desc='keyword processing...')\n",
    "for path in pbar : \n",
    "    if not any(keyword in path for keyword in category) or any(keyword in path for keyword in delete) : continue\n",
    "    df_row = {}\n",
    "    df_row[\"data_dir\"] = path\n",
    "    string = path\n",
    "    \n",
    "    if \"_\" in string : string = string.replace(\"_\", \" \") \n",
    "    if \"reverse crash\" in string : string = string.replace(\"reverse crash\", \"reverse_crash\")\n",
    "    \n",
    "    chs = [\"closed hat\", \"close hat\", \"closed hihat\", \"close hihat\", \"hat closed\", \"hihat closed\",\n",
    "           \"closed hats\", \"close hats\", \"closed hihats\", \"close hihats\", \"hats closed\", \"hihats closed\",\n",
    "          \"closed hh\", \"clhh\", \"ukg ch\", \"2sg ch\"]\n",
    "    ohs = [\"opened hat\", \"open hat\", \"opened hihat\", \"open hihat\", \"hat opened\", \"hihat opened\",\n",
    "           \"opened hats\", \"open hats\", \"opened hihats\", \"open hihats\", \"hats opened\", \"hihats opened\",\n",
    "           \"open hh\", \"ophh\", \"ukg oh\", \"2sg oh\"]\n",
    "    \n",
    "    for ch in chs :\n",
    "        string = string.replace(ch, \"closed_hat\")\n",
    "    for oh in ohs :\n",
    "        string = string.replace(oh, \"open_hat\")\n",
    "    \n",
    "    if \"snr\" in string : string = string.replace(\"snr\", \"snare\")\n",
    "    if \"percs\" in string : string = string.replace(\"percs\", \"percussion\")\n",
    "    if \"crsh\" in string : string = string.replace(\"crsh\", \"crash\")\n",
    "    if \"prc\" in string : string = string.replace(\"prc\", \"percussion\")\n",
    "    if \"clp\" in string : string = string.replace(\"clp\", \"clap\")\n",
    "    if \"kawaii bass\" in string : string = string.replace(\"kawaii bass\", \"future bass\")\n",
    "    if \"clap & snares\" in string : string = string.replace(\"clap & snares\", \"\")\n",
    "    if \"snares and claps\" in string : string = string.replace(\"snares and claps\", \"\")\n",
    "    if \"pop_\" in string : string = string.replace(\"pop_\", \"pop \")\n",
    "        \n",
    "    specialChars = \"\\.-,+*%&@^\"\n",
    "    for char in specialChars :\n",
    "        string = string.replace(char, ' ')\n",
    "    \n",
    "    types = []\n",
    "    for type_ in category :\n",
    "        if type_ in string :\n",
    "            types.append(type_)\n",
    "    type_ = \",\".join(types)\n",
    "    df_row[\"category\"] = type_\n",
    "    \n",
    "    \n",
    "    gens = []\n",
    "    if \"benzie\" in string :\n",
    "        gens.append(\"trap\"); gens.append(\"hip_hop\")\n",
    "    if \"hbd\" in string :\n",
    "        gens.append(\"dubstep\")\n",
    "        \n",
    "    for genre in genres :\n",
    "        genre_ = genre.replace(\" \", \"_\")\n",
    "        if genre_ == \"pop_\" : genre_ = \"pop\"\n",
    "        if genre in string :\n",
    "            string = string.replace(genre, genre_)\n",
    "            gens.append(genre_)\n",
    "    if \"aethral\" in string :\n",
    "        if not \"hardcore\" in gens :\n",
    "            gens.append(\"hardcore\")\n",
    "    genre_ = \",\".join(gens)\n",
    "    df_row[\"genre\"] = genre_\n",
    "    \n",
    "    \n",
    "    sty = []\n",
    "    for style_ in style :\n",
    "        if style_ in string :\n",
    "            sty.append(style_)\n",
    "    style_ = \",\".join(sty)\n",
    "    df_row[\"style\"] = style_\n",
    "    \n",
    "    \n",
    "    df = df.append(df_row, ignore_index=True)\n",
    "    df.replace(np.nan, '', regex=True)\n",
    "\n",
    "        \n",
    "    strlist = sorted(list(filter(None, string.split(' ')[4:-1])))\n",
    "    \n",
    "    for i in range(len(strlist)) :\n",
    "        if i>=1 and strlist[i-1] == strlist[i] :\n",
    "            continue\n",
    "        word = strlist[i]\n",
    "        keywords[word] = keywords.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde42752",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keywords = sorted(keywords.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(len(keywords))\n",
    "#print(keywords)\n",
    "\n",
    "for key in keywords :\n",
    "    if key[1] <= 5 : pass\n",
    "    print(key[0], \" : \" , key[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "52c01525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_dir</th>\n",
       "      <th>category</th>\n",
       "      <th>genre</th>\n",
       "      <th>style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d:\\music\\sample packs\\1. delifb chill future b...</td>\n",
       "      <td>snare</td>\n",
       "      <td>future_bass,trap</td>\n",
       "      <td>future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d:\\music\\sample packs\\1. delifb chill future b...</td>\n",
       "      <td>tom</td>\n",
       "      <td>future_bass,trap</td>\n",
       "      <td>future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d:\\music\\sample packs\\1. delifb chill future b...</td>\n",
       "      <td>drum</td>\n",
       "      <td>future_bass,trap</td>\n",
       "      <td>future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d:\\music\\sample packs\\1. delifb chill future b...</td>\n",
       "      <td>drum</td>\n",
       "      <td>future_bass,trap</td>\n",
       "      <td>future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d:\\music\\sample packs\\1. delifb chill future b...</td>\n",
       "      <td>drum</td>\n",
       "      <td>future_bass,trap</td>\n",
       "      <td>future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9374</th>\n",
       "      <td>d:\\music\\sample packs\\w.a.prod mega pack\\wapro...</td>\n",
       "      <td>snare</td>\n",
       "      <td>bass_house,house</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9375</th>\n",
       "      <td>d:\\music\\sample packs\\w.a.prod mega pack\\wapro...</td>\n",
       "      <td>snare</td>\n",
       "      <td>bass_house,house</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9376</th>\n",
       "      <td>d:\\music\\sample packs\\w.a.prod mega pack\\wapro...</td>\n",
       "      <td>snare</td>\n",
       "      <td>bass_house,house</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9377</th>\n",
       "      <td>d:\\music\\sample packs\\w.a.prod mega pack\\wapro...</td>\n",
       "      <td>snare</td>\n",
       "      <td>bass_house,house</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9378</th>\n",
       "      <td>d:\\music\\sample packs\\w.a.prod mega pack\\wapro...</td>\n",
       "      <td>snare</td>\n",
       "      <td>bass_house,house</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9379 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               data_dir category  \\\n",
       "0     d:\\music\\sample packs\\1. delifb chill future b...    snare   \n",
       "1     d:\\music\\sample packs\\1. delifb chill future b...      tom   \n",
       "2     d:\\music\\sample packs\\1. delifb chill future b...     drum   \n",
       "3     d:\\music\\sample packs\\1. delifb chill future b...     drum   \n",
       "4     d:\\music\\sample packs\\1. delifb chill future b...     drum   \n",
       "...                                                 ...      ...   \n",
       "9374  d:\\music\\sample packs\\w.a.prod mega pack\\wapro...    snare   \n",
       "9375  d:\\music\\sample packs\\w.a.prod mega pack\\wapro...    snare   \n",
       "9376  d:\\music\\sample packs\\w.a.prod mega pack\\wapro...    snare   \n",
       "9377  d:\\music\\sample packs\\w.a.prod mega pack\\wapro...    snare   \n",
       "9378  d:\\music\\sample packs\\w.a.prod mega pack\\wapro...    snare   \n",
       "\n",
       "                 genre   style  \n",
       "0     future_bass,trap  future  \n",
       "1     future_bass,trap  future  \n",
       "2     future_bass,trap  future  \n",
       "3     future_bass,trap  future  \n",
       "4     future_bass,trap  future  \n",
       "...                ...     ...  \n",
       "9374  bass_house,house          \n",
       "9375  bass_house,house          \n",
       "9376  bass_house,house          \n",
       "9377  bass_house,house          \n",
       "9378  bass_house,house          \n",
       "\n",
       "[9379 rows x 4 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(\"metadata.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb27b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(num, dim, max_norm=1) :\n",
    "    return nn.Embedding(num_embeddings=num, embedding_dim=dim, max_norm=max_norm)\n",
    "\n",
    "def dense_layer(input_, output_) :\n",
    "    return nn.Linear(input_, output_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "3450bc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'drum': 1,\n",
       "  'kick': 2,\n",
       "  'snare': 3,\n",
       "  'foley': 4,\n",
       "  'percussion': 5,\n",
       "  'clap': 6,\n",
       "  'cymbal': 7,\n",
       "  'crash': 8,\n",
       "  'tom': 9,\n",
       "  'rim': 10,\n",
       "  'bell': 11,\n",
       "  'tambourine': 12,\n",
       "  'hat': 13,\n",
       "  'closed_hat': 14,\n",
       "  'open_hat': 15,\n",
       "  'ride': 16,\n",
       "  'depot': 17,\n",
       "  'snap': 18,\n",
       "  'shake': 19,\n",
       "  'reverse_crash': 20,\n",
       "  'impact': 21,\n",
       "  None: 0},\n",
       " {'future_bass': 1,\n",
       "  'dubstep': 2,\n",
       "  'hitech': 3,\n",
       "  'trap': 4,\n",
       "  'hardcore': 5,\n",
       "  'riddim': 6,\n",
       "  'uk_garage': 7,\n",
       "  'acoustic': 8,\n",
       "  'bass_house': 9,\n",
       "  'pop': 10,\n",
       "  'house': 11,\n",
       "  'future_bounce': 12,\n",
       "  'future_house': 13,\n",
       "  'tropical_house': 14,\n",
       "  'edm': 15,\n",
       "  'rnb': 16,\n",
       "  'reggae': 17,\n",
       "  'hip_hop': 18,\n",
       "  'hardstyle': 19,\n",
       "  'orchestral': 20,\n",
       "  'cinematic': 21,\n",
       "  None: 0},\n",
       " {'kshmr': 1,\n",
       "  'future': 2,\n",
       "  'acoustic': 3,\n",
       "  'spicy': 4,\n",
       "  'tropical': 5,\n",
       "  'high': 6,\n",
       "  'low': 7,\n",
       "  'electro': 8,\n",
       "  '808': 9,\n",
       "  'melodic': 10,\n",
       "  'weird': 11,\n",
       "  'punch': 12,\n",
       "  'kawaii': 13,\n",
       "  'crunch': 14,\n",
       "  'epic': 15,\n",
       "  'wet': 16,\n",
       "  'lofi': 17,\n",
       "  'deep': 18,\n",
       "  'fat': 19,\n",
       "  'tight': 20,\n",
       "  'vintage': 21,\n",
       "  'top': 22,\n",
       "  'gated': 23,\n",
       "  'closet': 24,\n",
       "  'pan': 25,\n",
       "  'heavy': 26,\n",
       "  'light': 27,\n",
       "  'scratch': 28,\n",
       "  'roll': 29,\n",
       "  'noise': 30,\n",
       "  'hard': 31,\n",
       "  'soft': 32,\n",
       "  'stomp': 33,\n",
       "  'short': 34,\n",
       "  'thick': 35,\n",
       "  'anime': 36,\n",
       "  'bright': 37,\n",
       "  'glitch': 38,\n",
       "  'beatbox': 39,\n",
       "  'dirty': 40,\n",
       "  'distort': 41,\n",
       "  'droplet': 42,\n",
       "  'reverse': 43,\n",
       "  'rattle': 44,\n",
       "  'djembe': 45,\n",
       "  'chime': 46,\n",
       "  'stadium': 47,\n",
       "  None: 0})"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_category = {cat: i+1 for i, cat in enumerate(category)}\n",
    "vocab_category[None] = 0\n",
    "vocab_category\n",
    "vocab_genre = {cat: i+1 for i, cat in enumerate(genres)}\n",
    "vocab_genre[None] = 0\n",
    "vocab_style    = {sty: i+1 for i, sty in enumerate(style)}\n",
    "vocab_style[None] = 0\n",
    "\n",
    "vocab = vocab_category, vocab_genre, vocab_style\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "7a4e972c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.7074, -0.8653,  1.3687,  ...,  0.3700,  0.7455,  1.1002],\n",
      "        [-0.4378,  0.4857, -0.4466,  ..., -0.4717,  0.8363, -1.8643],\n",
      "        [ 0.5234, -0.3202, -0.2012,  ...,  2.6093,  0.4220, -1.6755],\n",
      "        ...,\n",
      "        [ 0.8522, -0.1339, -0.4843,  ..., -0.8751, -1.1285,  0.4790],\n",
      "        [ 0.2440, -0.8524,  0.3826,  ..., -1.1311, -0.3999, -0.3026],\n",
      "        [ 0.1246,  1.5152,  1.3147,  ..., -1.9180, -0.6259, -1.8303]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "category_embedding = embedding(len(category), embedding_dim)\n",
    "category_table = category_embedding.weight\n",
    "#category_table[0] = torch.mean(category_table[1:], 0)\n",
    "print(category_embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "9798cd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\music\\sample packs\\future house essentials - sample pack\\10.ultrasonic - snares\\big snares\\ultrasonic - future house essentials - big snare 4.wav\n",
      "['snare']\n",
      "torch.Size([65875])\n",
      "torch.Size([65875])\n",
      "torch.Size([70285])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "data = df.loc[3234, :]\n",
    "print(data[\"data_dir\"])\n",
    "print(data[\"category\"].split(\",\"))\n",
    "cat_list = data[\"category\"].split(\",\")\n",
    "sample, _ = torchaudio.load(data[\"data_dir\"], channels_first=True)\n",
    "print(sample[0].shape)\n",
    "sample_mono = torch.mean(sample, 0)\n",
    "print(sample_mono.shape)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "sample = F.pad(sample_mono, (4410, 0), \"constant\")\n",
    "print(sample.shape)\n",
    "print(sample[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "a20dd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "    def override(self, attrs):\n",
    "        if isinstance(attrs, dict):\n",
    "            self.__dict__.update(**attrs)\n",
    "        elif isinstance(attrs, (list, tuple, set)):\n",
    "            for attr in attrs:\n",
    "              self.override(attr)\n",
    "        elif attrs is not None:\n",
    "            raise NotImplementedError\n",
    "        return self\n",
    "\n",
    "\n",
    "params = AttrDict(\n",
    "    # Training params\n",
    "    batch_size=16,\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=None,\n",
    "\n",
    "    # Data params\n",
    "    sample_rate=44100,\n",
    "    n_mels=80,\n",
    "    n_fft=1024,\n",
    "    hop_samples=256,\n",
    "    crop_mel_frames=62,  # Probably an error in paper.\n",
    "\n",
    "    # Model params\n",
    "    residual_layers=30,\n",
    "    residual_channels=64,\n",
    "    dilation_cycle_length=10,\n",
    "    unconditional = True,\n",
    "    noise_schedule=np.linspace(1e-4, 0.05, 50).tolist(),\n",
    "    inference_noise_schedule=[0.0001, 0.001, 0.01, 0.05, 0.2, 0.5],\n",
    "    tag_padding_len = 5,\n",
    "\n",
    "    # unconditional sample len\n",
    "    audio_len = 44100*2, # unconditional_synthesis_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "0e685ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1551,  0.5584, -0.7237,  0.5822, -0.1113,  0.3201, -1.1290, -0.9714,\n",
       "         1.8785,  0.2439, -0.9322,  0.5658,  0.9985,  3.1132,  0.9098,  1.3735,\n",
       "        -0.5419, -1.4475, -1.0851, -1.3415,  0.2481, -1.2227, -1.4663, -1.5551,\n",
       "         0.1672, -0.5215, -0.9334, -0.4775,  0.7160,  0.1972, -0.9906, -1.5978,\n",
       "        -0.9777, -2.0815,  1.0768, -0.3359,  0.4107, -0.6729,  0.2711, -0.8761,\n",
       "         0.5261, -0.7181,  1.4017, -0.5142,  0.2799,  0.2181,  1.6918, -2.2408,\n",
       "        -0.9539,  0.3914, -0.2024, -1.3263,  0.6319, -0.4128,  0.9235,  1.7229,\n",
       "         0.0792, -0.3771, -0.6672, -2.1017, -1.9712, -0.3261,  0.2795,  1.2892,\n",
       "        -0.3883, -0.6403,  0.0078,  0.7129,  0.6458,  0.0064,  1.1885, -0.4342,\n",
       "        -1.1530,  0.6996, -0.5368, -1.1166, -0.8037,  0.5226, -0.2905, -1.0453,\n",
       "        -0.2389, -0.3354,  0.0666,  0.2247, -0.8076, -0.1967,  0.5220,  0.4989,\n",
       "        -0.0425,  0.7453, -0.5164,  0.9381, -0.7303, -0.2670,  1.7628,  0.7586,\n",
       "         0.0202, -1.7915,  1.8927, -1.3792], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def category_embedding(table, cat_list) :\n",
    "    vectors = []\n",
    "    for category in cat_list :\n",
    "        idx = vocab_category[category]\n",
    "        vectors.append(table[idx])\n",
    "    embedded_vector = torch.stack(vectors, dim=0)\n",
    "    return torch.mean(embedded_vector, 0)\n",
    "    \n",
    "category_embedding(category_table, cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "ec7d6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class SampleDataset(torch.utils.data.Dataset) :\n",
    "    def __init__(self, metadata_dir, params, df=pd.DataFrame(), size=320) :\n",
    "        super().__init__()\n",
    "        self.metadata_dir = metadata_dir\n",
    "        self.params = params\n",
    "        self.df = df\n",
    "        \n",
    "        if (self.df.index == 0).all() :\n",
    "            self.df = pd.read_csv(self.metadata_dir+\"metadata.csv\")\n",
    "        self.df = self.df.replace(np.nan, '', regex=True)\n",
    "        start = np.random.randint(len(self.df) - size + 1)\n",
    "        self.df = self.df[start : start + size].reset_index()\n",
    "        print(start)\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx : int) :\n",
    "        metadata = self.df.loc[idx, :]\n",
    "        \n",
    "        sample_path = metadata[\"data_dir\"]\n",
    "        categories = metadata[\"category\"].split(\",\")\n",
    "        genres = metadata[\"genre\"].split(\",\")\n",
    "        styles = metadata[\"style\"].split(\",\")\n",
    "        \n",
    "        signal, _ = torchaudio.load(sample_path, channels_first=True)\n",
    "        signal = torch.mean(signal, 0)\n",
    "        signal = F.pad(signal, (4410, 0), mode=\"constant\", value=0)\n",
    "        \n",
    "        if len(signal) < self.params.audio_len :\n",
    "            signal = F.pad(signal, (0, self.params.audio_len - len(signal)), mode=\"constant\", value=0)\n",
    "        start = random.randint(0, signal.shape[-1] - self.params.audio_len)\n",
    "        end = start + self.params.audio_len\n",
    "        signal = signal[start:end]\n",
    "        \n",
    "        return {\n",
    "            'audio' : signal,\n",
    "            #'spectrogram' : None,\n",
    "            'category' : categories,\n",
    "            'genre' : genres,\n",
    "            'style' : styles\n",
    "        }\n",
    "\n",
    "\n",
    "class Collator:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "    \n",
    "    def padding(self, list_) :\n",
    "        pad_length = self.params.tag_padding_len\n",
    "        if len(list_) < pad_length :\n",
    "            while len(list_) != pad_length :\n",
    "                list_.append('')\n",
    "        else :\n",
    "            list_ = list_[:pad_length]\n",
    "        return list_\n",
    "\n",
    "    def collate(self, minibatch):\n",
    "        samples_per_frame = self.params.hop_samples\n",
    "        for sample in minibatch:\n",
    "            if self.params.unconditional:\n",
    "                sample['audio'] = F.pad(sample['audio'], (4410, 0), mode=\"constant\", value=0)\n",
    "                if len(sample['audio']) < self.params.audio_len:\n",
    "                    sample['audio'] = F.pad(sample['audio'], (0, self.params.audio_len-len(sample['audio'])))\n",
    "\n",
    "                start = random.randint(0, sample['audio'].shape[-1] - self.params.audio_len)\n",
    "                end = start + self.params.audio_len\n",
    "                sample['audio'] = sample['audio'][start:end]\n",
    "                sample['audio'] = F.pad(sample['audio'], (0, (end - start) - len(sample['audio'])), mode='constant', value=0)\n",
    "            else:\n",
    "                sample['audio'] = F.pad(sample['audio'], (4410, 0), mode=\"constant\", value=0)\n",
    "                sample['spectrogram'] = F.pad(sample['spectrogram'], (6, 0), mode=\"constant\", value=0)\n",
    "                if len(sample['spectrogram']) < self.params.crop_mel_frames:\n",
    "                    sample['audio'] = F.pad(sample['audio'], (0, self.params.audio_len-len(sample['audio'])))\n",
    "                    sample['spectrogram'] = F.pad(sample['spectrogram'], (0, self.params.crop_mel_frames-len(sample['spectrogram'])))\n",
    "\n",
    "                start = random.randint(0, sample['spectrogram'].shape[0] - self.params.crop_mel_frames)\n",
    "                end = start + self.params.crop_mel_frames\n",
    "                sample['spectrogram'] = sample['spectrogram'][start:end].T\n",
    "                \n",
    "                start *= samples_per_frame\n",
    "                end *= samples_per_frame\n",
    "                sample['audio'] = sample['audio'][start:end]\n",
    "                sample['audio'] = np.pad(sample['audio'], (0, (end-start) - len(sample['audio'])), mode='constant')\n",
    "\n",
    "        audio = torch.stack([sample['audio'] for sample in minibatch if 'audio' in sample])\n",
    "        category = np.stack([self.padding(sample['category']) for sample in minibatch if 'audio' in sample])\n",
    "        genre = np.stack([self.padding(sample['genre']) for sample in minibatch if 'audio' in sample])\n",
    "        style = np.stack([self.padding(sample['style']) for sample in minibatch if 'audio' in sample])\n",
    "        if self.params.unconditional:\n",
    "            return {\n",
    "                'audio': audio,\n",
    "                #'spectrogram': None,\n",
    "                'category': category,\n",
    "                'genre': genre,\n",
    "                'style': style\n",
    "            }\n",
    "        spectrogram = torch.stack([sample['spectrogram'] for sample in minibatch if 'spectrogram' in sample])\n",
    "        return {\n",
    "            'audio': audio,\n",
    "            'spectrogram': spectrogram,\n",
    "            'category': category,\n",
    "            'genre': genre,\n",
    "            'style': style\n",
    "        }\n",
    "\n",
    "\n",
    "def from_path(data_dir, params, is_distributed=False):\n",
    "    \"\"\"if params.unconditional:\n",
    "        dataset = UnconditionalDataset(data_dirs)\n",
    "    else:#with condition\n",
    "        dataset = ConditionalDataset(data_dirs)\"\"\"\n",
    "    dataset = SampleDataset(data_dir, params)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=params.batch_size,\n",
    "        collate_fn=Collator(params).collate,\n",
    "        shuffle=not is_distributed,\n",
    "        num_workers=os.cpu_count(),\n",
    "        sampler=DistributedSampler(dataset) if is_distributed else None,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "fbfdefdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3293\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001C00864A608>\n",
      "{'audio': tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.4669e-04,\n",
      "          1.2100e-04,  9.4593e-05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.0518e-05,\n",
      "         -3.0518e-05, -3.0518e-05]]), 'category': array([['drum', 'percussion', '', '', ''],\n",
      "       ['cymbal', 'crash', 'reverse_crash', '', ''],\n",
      "       ['cymbal', 'ride', '', '', ''],\n",
      "       ['foley', '', '', '', ''],\n",
      "       ['cymbal', 'hat', 'closed_hat', '', ''],\n",
      "       ['cymbal', 'crash', '', '', ''],\n",
      "       ['clap', '', '', '', ''],\n",
      "       ['cymbal', 'hat', '', '', ''],\n",
      "       ['snare', '', '', '', ''],\n",
      "       ['drum', 'kick', 'percussion', '', ''],\n",
      "       ['cymbal', 'ride', '', '', ''],\n",
      "       ['kick', '', '', '', ''],\n",
      "       ['clap', '', '', '', ''],\n",
      "       ['cymbal', 'tambourine', 'shake', '', ''],\n",
      "       ['clap', '', '', '', ''],\n",
      "       ['cymbal', 'crash', '', '', '']], dtype='<U13'), 'genre': array([['cinematic', '', '', '', ''],\n",
      "       ['house', 'future_house', '', '', ''],\n",
      "       ['house', 'future_house', '', '', ''],\n",
      "       ['cinematic', '', '', '', ''],\n",
      "       ['hitech', '', '', '', ''],\n",
      "       ['hitech', '', '', '', ''],\n",
      "       ['hitech', '', '', '', ''],\n",
      "       ['house', 'future_house', '', '', ''],\n",
      "       ['hitech', '', '', '', ''],\n",
      "       ['cinematic', '', '', '', ''],\n",
      "       ['house', 'future_house', '', '', ''],\n",
      "       ['hitech', '', '', '', ''],\n",
      "       ['hitech', '', '', '', ''],\n",
      "       ['house', 'future_house', '', '', ''],\n",
      "       ['hitech', '', '', '', ''],\n",
      "       ['hitech', '', '', '', '']], dtype='<U12'), 'style': array([['low', '', '', '', ''],\n",
      "       ['future', 'reverse', '', '', ''],\n",
      "       ['future', '', '', '', ''],\n",
      "       ['', '', '', '', ''],\n",
      "       ['wet', '', '', '', ''],\n",
      "       ['wet', '', '', '', ''],\n",
      "       ['scratch', '', '', '', ''],\n",
      "       ['future', '', '', '', ''],\n",
      "       ['scratch', '', '', '', ''],\n",
      "       ['', '', '', '', ''],\n",
      "       ['future', '', '', '', ''],\n",
      "       ['scratch', '', '', '', ''],\n",
      "       ['wet', '', '', '', ''],\n",
      "       ['future', '', '', '', ''],\n",
      "       ['scratch', '', '', '', ''],\n",
      "       ['', '', '', '', '']], dtype='<U7')}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:\\\\Study\\\\인턴\\\\Intern project\\\\\"+\"metadata.csv\")\n",
    "df = df.replace(np.nan, '', regex=True)\n",
    "\n",
    "dataset = SampleDataset(\"D:\\\\Study\\\\인턴\\\\Intern project\\\\\", params, df=df)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=16,\n",
    "        collate_fn=Collator(params).collate,\n",
    "        shuffle=True,\n",
    "        sampler=None,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "print(dataloader)\n",
    "#dataset = from_path(\"D:\\\\Study\\\\인턴\\\\Intern project\\\\\", params)\n",
    "iterator = iter(dataloader)\n",
    "print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "9a63f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding = nn.Embedding\n",
    "@torch.jit.script\n",
    "def silu(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "class EmbeddingTable :\n",
    "    def __init__(self, vocab, residual_channels) :\n",
    "        self.vocab = vocab\n",
    "        self.vocab_category = vocab[0]\n",
    "        self.vocab_genre = vocab[1]\n",
    "        self.vocab_style = vocab[2]\n",
    "        self.embedding_dim = residual_channels\n",
    "\n",
    "    def category_table(self) :\n",
    "        num = len(self.vocab_category)\n",
    "        dim = self.embedding_dim\n",
    "        return Embedding(num_embeddings=num, embedding_dim=dim)\n",
    "\n",
    "    def genre_table(self) :\n",
    "        num = len(self.vocab_genre)\n",
    "        dim = self.embedding_dim\n",
    "        return Embedding(num_embeddings=num, embedding_dim=dim)\n",
    "\n",
    "    def style_table(self) :\n",
    "        num = len(self.vocab_style)\n",
    "        dim = self.embedding_dim\n",
    "        return Embedding(num_embeddings=num, embedding_dim=dim)\n",
    "\n",
    "    def make_table(self) :\n",
    "        cat_table = self.category_table().weight\n",
    "        gen_table = self.genre_table().weight\n",
    "        sty_table = self.style_table().weight\n",
    "        return cat_table, gen_table, sty_table\n",
    "\n",
    "\n",
    "# Tag embedding\n",
    "class TagEmbedding(nn.Module) :\n",
    "    def __init__(self, residual_channels, vocab) :\n",
    "        super().__init__()\n",
    "        self.residual_channels = residual_channels\n",
    "\n",
    "        self.vocab_category = vocab[0]\n",
    "        self.vocab_genre = vocab[1]\n",
    "        self.vocab_style = vocab[2]\n",
    "\n",
    "        self.embedding_table = EmbeddingTable(vocab, residual_channels)\n",
    "        self.cat_table, self.gen_table, self.sty_table = self.embedding_table.make_table()\n",
    "        self.register_buffer('category embedding', self.cat_table, persistent=False)\n",
    "        self.register_buffer('genre    embedding', self.gen_table, persistent=False)\n",
    "        self.register_buffer('style    embedding', self.sty_table, persistent=False)\n",
    "        \n",
    "        self.projection1 = Linear(residual_channels, residual_channels)\n",
    "        self.projection2 = Linear(residual_channels, residual_channels)\n",
    "        self.projection3 = Linear(residual_channels, residual_channels)\n",
    "\n",
    "    def category_embedding(self, list_) :\n",
    "        embedded = []\n",
    "        for vector in list_ :\n",
    "            vectors = []\n",
    "            for category in vector :\n",
    "                try :\n",
    "                    idx = self.vocab_category[category]\n",
    "                except :\n",
    "                    idx = self.vocab_category[None]\n",
    "                vectors.append(self.cat_table[idx])\n",
    "            embedded.append(torch.mean(torch.stack(vectors, dim=0), 0))\n",
    "        return torch.stack(embedded, 0) \n",
    "\n",
    "    def genre_embedding(self, list_) :\n",
    "        embedded = []\n",
    "        for vector in list_ :\n",
    "            vectors = []\n",
    "            for genre in vector :\n",
    "                try :\n",
    "                    idx = self.vocab_genre[genre]\n",
    "                except :\n",
    "                    idx = self.vocab_genre[None]\n",
    "                vectors.append(self.gen_table[idx])\n",
    "            embedded.append(torch.mean(torch.stack(vectors, dim=0), 0))\n",
    "        return torch.stack(embedded, 0) \n",
    "\n",
    "    def style_embedding(self, list_) :\n",
    "        embedded = []\n",
    "        for vector in list_ :\n",
    "            vectors = []\n",
    "            for style in vector :\n",
    "                try :\n",
    "                    idx = self.vocab_style[style]\n",
    "                except :\n",
    "                    idx = self.vocab_style[None]                \n",
    "                vectors.append(self.sty_table[idx])\n",
    "            embedded.append(torch.mean(torch.stack(vectors, dim=0), 0))\n",
    "        return torch.stack(embedded, 0) \n",
    "    \n",
    "    def forward(self, data) :\n",
    "        category = data[\"category\"]\n",
    "        genre    = data[\"genre\"]\n",
    "        style    = data[\"style\"]\n",
    "\n",
    "        embedded_cat = silu(self.projection1(self.category_embedding(category)))\n",
    "        embedded_gen = silu(self.projection2(self.genre_embedding(genre)))\n",
    "        embedded_sty = silu(self.projection3(self.style_embedding(style)))\n",
    "\n",
    "        return embedded_cat + embedded_gen + embedded_sty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "d09171e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                           data_dir  \\\n",
      "0              0  d:\\music\\sample packs\\1. delifb chill future b...   \n",
      "1              1  d:\\music\\sample packs\\1. delifb chill future b...   \n",
      "2              2  d:\\music\\sample packs\\1. delifb chill future b...   \n",
      "3              3  d:\\music\\sample packs\\1. delifb chill future b...   \n",
      "4              4  d:\\music\\sample packs\\1. delifb chill future b...   \n",
      "...          ...                                                ...   \n",
      "1995        1995  d:\\music\\sample packs\\cymatics samples\\cymatic...   \n",
      "1996        1996  d:\\music\\sample packs\\cymatics samples\\cymatic...   \n",
      "1997        1997  d:\\music\\sample packs\\cymatics samples\\cymatic...   \n",
      "1998        1998  d:\\music\\sample packs\\cymatics samples\\cymatic...   \n",
      "1999        1999  d:\\music\\sample packs\\cymatics samples\\cymatic...   \n",
      "\n",
      "              category             genre   style  \n",
      "0                snare  future_bass,trap  future  \n",
      "1                  tom  future_bass,trap  future  \n",
      "2                 drum  future_bass,trap  future  \n",
      "3                 drum  future_bass,trap  future  \n",
      "4                 drum  future_bass,trap  future  \n",
      "...                ...               ...     ...  \n",
      "1995  drum,foley,depot                            \n",
      "1996  drum,foley,depot                            \n",
      "1997             foley                       pan  \n",
      "1998             foley                       pan  \n",
      "1999             foley                       pan  \n",
      "\n",
      "[2000 rows x 5 columns]\n",
      "9379\n",
      "torch.Size([16, 128, 1])\n"
     ]
    }
   ],
   "source": [
    "data = next(iterator)\n",
    "tag_embedding = TagEmbedding(params.residual_channels, vocab)\n",
    "data.pop('audio')\n",
    "tag_projection = Conv1d(params.residual_channels, 2 * params.residual_channels, 1)\n",
    "print(df[:2000])\n",
    "print(len(df))\n",
    "print(tag_projection(tag_embedding(data).unsqueeze(-1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "96d12956",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear = nn.Linear\n",
    "ConvTranspose2d = nn.ConvTranspose2d\n",
    "\n",
    "\n",
    "def Conv1d(*args, **kwargs):\n",
    "    layer = nn.Conv1d(*args, **kwargs)\n",
    "    nn.init.kaiming_normal_(layer.weight)\n",
    "    return layer\n",
    "\n",
    "class DiffusionEmbedding(nn.Module):\n",
    "    def __init__(self, max_steps):\n",
    "        super().__init__()\n",
    "        self.register_buffer('embedding', self._build_embedding(max_steps), persistent=False)\n",
    "        self.projection1 = Linear(128, 512)\n",
    "        self.projection2 = Linear(512, 512)\n",
    "\n",
    "    def forward(self, diffusion_step):\n",
    "        if diffusion_step.dtype in [torch.int32, torch.int64]:\n",
    "            x = self.embedding[diffusion_step]\n",
    "        else:\n",
    "            x = self._lerp_embedding(diffusion_step)\n",
    "        x = self.projection1(x)\n",
    "        x = silu(x)\n",
    "        x = self.projection2(x)\n",
    "        x = silu(x)\n",
    "        return x\n",
    "\n",
    "    def _lerp_embedding(self, t):\n",
    "        low_idx = torch.floor(t).long()\n",
    "        high_idx = torch.ceil(t).long()\n",
    "        low = self.embedding[low_idx]\n",
    "        high = self.embedding[high_idx]\n",
    "        return low + (high - low) * (t - low_idx)\n",
    "\n",
    "    def _build_embedding(self, max_steps):\n",
    "        steps = torch.arange(max_steps).unsqueeze(1)  # [T,1]\n",
    "        dims = torch.arange(64).unsqueeze(0)          # [1,64]\n",
    "        table = steps * 10.0**(dims * 4.0 / 63.0)     # [T,64]\n",
    "        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6f57d04b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Kang Minyeong\\AppData\\Local\\Temp\\ipykernel_7888\\3846583045.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdiffembed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiffusionEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiffembed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Kang Minyeong\\anaconda3\\envs\\AIP2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kang Minyeong\\AppData\\Local\\Temp\\ipykernel_7888\\725633631.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, diffusion_step)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiffusion_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdiffusion_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdiffusion_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lerp_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiffusion_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "diffembed = DiffusionEmbedding(1000)\n",
    "print(diffembed(torch.IntTensor(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ec41dea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.9005,  2.4372, -2.3213,  0.4591, -0.3220],\n",
      "         [ 0.0825, -2.0842, -2.9535,  0.0303, -0.4367]]]) \n",
      " tensor([[[1.1981],\n",
      "         [1.5115]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 2, 5)\n",
    "y = torch.randn(1, 2, 1)\n",
    "print(x,'\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "30f4ac7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0987,  3.6354, -1.1232,  1.6572,  0.8762],\n",
       "         [ 1.5940, -0.5727, -1.4420,  1.5418,  1.0748]]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6de40867",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = nn.ConvTranspose2d(1, 1,  [3, 32], stride=[1, 16], padding=[1, 8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b8da5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.randn(16, 1, 80, 344)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a08dda66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 80, 88064])\n"
     ]
    }
   ],
   "source": [
    "batch = conv2(conv2(batch))\n",
    "batch = torch.squeeze(batch, 1)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "450f3e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344.53125\n"
     ]
    }
   ],
   "source": [
    "print(88200/256)\n",
    "conv = nn.Conv1d(80, 128, 1)\n",
    "batch = conv(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "70ec61c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba = torch.randn(16, 128, 88200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2b7fb6c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (88200) must match the size of tensor b (88064) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Kang Minyeong\\AppData\\Local\\Temp\\ipykernel_7888\\2002492648.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mba\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (88200) must match the size of tensor b (88064) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "ba + batchx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "27ed8a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soundfile'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(torchaudio.get_audio_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "ff7d2bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0088, -0.0039, -0.0442,  ...,  0.3424,  0.3377,  0.2730]])\n"
     ]
    }
   ],
   "source": [
    "wave, sr = torchaudio.load(\"./inference_test.wav\")\n",
    "print(wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "abc0b1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c083266b08>]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAyKElEQVR4nO3dd3wUZf4H8M+XQOidUAyBUEKTJgREkQ4KqIddVBA9Fb2D82ynWM96cvpTORVBVCxnQfRQUBAkIL0GCAEESYAAIUBC7yXJ9/fHziazu7O7MzszWzLf9+vFi91p+0yyme88z/N9niFmhhBCCOcqF+kCCCGEiCwJBEII4XASCIQQwuEkEAghhMNJIBBCCIcrH+kChKJevXqcnJwc6WIIIURMWbdu3SFmTvBeHpOBIDk5Genp6ZEuhhBCxBQi2q21XJqGhBDC4SQQCCGEw0kgEEIIh5NAIIQQDieBQAghHM6SQEBEU4kon4g2+1lPRPQuEWUTUSYRdVGtG0xEfyjrxllRHiGEEPpZVSP4DMDgAOuHAEhR/o0GMAkAiCgOwERlfTsAdxBRO4vKJIQQQgdLAgEzLwFwJMAmwwB8wS6rANQiokYAugPIZuadzHwBwDRl24iZt+UA8k+ei2QRPKzbfRRFxdExVfjczQdw6NR53dsfOH4Oi/7It7FE2o6fvYifNubhzIVCzFifi7I01Toz+5zPuYtF+H6d73muzTmCI6cvGDr+1v0nsG730ZDLNjNjH85dLApp/0AOnTqPuZv3W35ctaJixg8bclEc4O9t3e6jyMw9hunpe8vU9ypcfQSJAPaq3ucqy/wt90FEo4konYjSCwoKLCvYrkOnkTxuNtbtPorzhUV48L/rcOdHq0vW/5yZh+vfWxaRX/rqnYdx86QVmLx4h6H99h45g+NnL1pallPnC/HQl+tw9ydrdO8zbOIy3PPp2qDbLcs6hM37jpspnodHv83A377ZgNRX0/DY9I1Ym3MUvd5YiIm/Zeva/9zFIlwoLPZZviXvOPJPRPYm4br3lqH5M3M8lr316x944ruNWLjNM+jeOnklbvtwpaHjD/nPUtw8aYXhch04fg7Lsw/j79My0Ob5udhz+IzhYwRy76dr8dCX6y3/XgOu3/eT32/ExN+y8ei3G/HVas0xVwCAmyetwJ/eX44nv8/Ez5n2BqZwClcgII1lHGC570LmKcycysypCQk+I6RDtjTLFVR+3LAP7mv93iOlX+KxX2/AJgsvUkbsP+666Gw/eNLQfr3e+A1D/7M04Db7jp3FptzS81qWdQizlS92dv5Jn7si9wUw96j+P/CDJ/TVHkZ8shrXvbdM93GDyTt2FgBw5oLrzvT0hULsPXIWb877Q9f+bZ6fi+veW4rF2wswbc2ekuXXvrsMvd/8zbJyhmJL3gl435Pkn3T9nE+eK/TZPjv/lO1lyj95Dj1eX4Bnf9xUsuzTFbss/Yy9yvcu0N16qL5fl4vp6bl4e/52AMBhnbWoE+esCUp7Dp/B+F+2RbSGEa5AkAsgSfW+MYC8AMsd5fT5woBfqlC+H/uUi6E/PccvxPXvl158R3yyGmO+Xo9Nuccx8O0lmLJ0Z8m6/JPn0P+txQCAE14Xm/3Hz+J8ofVNAZG2/eApjJq6BuNmbPJYfu6ib03Baptyj2PqMmsvpHZyNz/tVtUCrLim/ZyZh+Rxs3HQ5lpYpBt4HvgiHZMX78COgtMRK0O4AsEsAHcr2UM9ABxn5v0A1gJIIaJmRBQPYLiyra2+X5eLaWv2hNwWarXur6Wh44u/+ix391XM2qg/Np4+73tXaIT7jj9jz7GSZTv9fEEvFhXjitcXovVzc019pvB0/fvL8PLPvxveLzP3uKEaW7T7dq2r1fiPA8ZqxGZNSMtC2+ft+05v2HMU63aXdqleLLb/5iIYq9JHvwGwEkBrIsolovuI6CEiekjZZA6AnQCyAXwE4K8AwMyFAMYCmAdgK4DpzLzFijIF8sR3GzFuxqaQ2kKt8NXq3Zi7+UDJ+9MXtO+o844ZvxP6RtWUYZXnf9TMCo6aTmynOKUK8lodslOX78JV/45s01UoCouKsfuw8bvhmRn78O6CrIDbnLlQiFsmrTAcTM56/XyLixlv/erZtKjVFHfuYhGy8wN/1o0frMDNk4z13djNqqyhO5i5ETNXYObGzPwJM09m5snKembmMczcgpk7MHO6at85zNxKWfeaFeUJ1derrb+Iann2h8146Mt1hvaZsT4XN0xcHnS7YlWd/PYPVyJ53GzTHXdZOtqZH/xveknbvJUOnTpvOOCE2izx31W7MTNjn8/yXzbtx2fLS5tqPly8w9Imi/yT53DgeOBjHTtzAe3/Oa/k/YP/Nfb9MWNZ1iF8tGRn8A1D9Nqcrejz5iLsP27s+/P3aRkl7freiosZRcWM1buOIH33UfxrzlYszz6EwROW+CQC7Dsa/HNX7TqM9xZ6JhuM/2Ubrnh9gceyx6ZnYODbSzyCdiyQkcWKnMOnQ6qOW8k71U/defTY9I3I2Hss4P4Hjp/DhLTSO6TVu1zVz3s+XYMjp10Xkg17PJvDrLqrn7flIK4cv9CSY7kdP3MRqa+m4bXZW43tF2JmyfM/bsbfp2X4LP/LV+vx4k+l343Xf9kGABg1VX8GlbdzF4tK7uq7v7YAPZQLilZ/y5a84yWJA26Lt5dmzmllXFhpxCer8docY7+Dr1bvRtrvB3VtO11pAvq/eZ4X9aVZhwC42vCPnTH2Ox37zXq08MquevaHTdh24KRP/9lJHZ2+2/Zr3+V7/15W7XT9zZ23KIXWfdNhR0qumgQCxcWi0ruE8xqpg+GgN6vl8KnzmhkGD0/bUJIpo3a+sBirdx7GqfOF+HCx551dcRTnQrs70H/9/UCQLT0dCOFOveCk/vERbkbGVHjr8sp8tNFoh1Y3GX65ajc+X5GDa99dFjAYendgL9h6ED0tDspGXSxi3P9FOtbtPoq7p67xuJAdOX0Bh1U/O3fT6MJtrsDBzFimBAEAPjcveszZ5P8789yPm5Bz6HRJNh7piKSRukn8WEkaOGFD2qya4wPBln0nIl2EEnuOBG8nzc4/ia6vpml+Mf3dNew7dhZ/+Wq96fK5BbsAFhYV4/VfjN1BAsDBE+fw1PeZPlX33KNnkWUwhdaDjljX7bW00I+v4fiZiyXpuN72HjmjGbABYIeqGe65Hzfjn7NcXWb+aoMzM/Zh7hbPi959n6cHzRoLl5snrcASJQ3XfbPV5ZX56PpqGs5eKEKb53/x2WfOpgMY8UnpWB6rb8yWZx9G3/9bhKvfWWLZMX/YkBv64MkouBdzfCD4Nn1v8I0UzIwJadtta/9bnn046DbuDJ5Pl+fYUgY9Ul8NfNFcuC3fp+bhj7pa/s+ZW/Bt+l60eu4XnyaSez8LPjDNn0CZNKfOF+KHDbkhHffQKf/55mO/WY8xX68v+exzF4twVGn66/WG/w7ddxfqG/TmlrY1+MUnM/cY/vLluoh27r/40+8+tZq9R89opuMG6mvyHjTntizrUNB+FqOOn7mIFTsOBd8QwKPfbtQ1eDIQPTUTuzg+ELgFayFhBr5bl4sJaVkenXZmREPWTbB+h0D8/cymp+u/sG4/qN0R7Z0xZeaPJNDP+U/vLcOj324M/eB+uDsg3Xezt3+4Epe9Mj/k45m5+Rjz9Xr8svmArk5RI5gZ/d9ahGlr9N1MLc8+hLOqmpB3zY/8/JLVGUWPf6f9uxrxyWoMfdc1iNKqaU0e+CIdd3602tDP3uiUHtFCAkEQ6u+m2eybomLGjoLSC1+wzA+74oT6i33rZM80NrNNSF+v3oO0rfo6Cb15N3FY5R1VB3ryuNkewW/nofAM4tmYqz06XWsqCyP0xMf9IaQh65GZexw7C07jsxU5urbPyj/lMTI7UHKEWqD2fjX38VbuKK1Z6+n78ddMtybniO5jmJGr1IB+81PbCQcJBH689NMWy0fMTkjbjgHKCF0AQS+Y/13lf84Tt/m/H0TesbNYYODi612jsXL+HH/pfN6YWal2218r8s4i+mljHoqL2fRFGHDlqZtRGIbBRIXKHUXvN3/DFytzLDtuKJksdl9UvT35fSYAV5ZVjp8buRnrfVOG1fr93yKriwUAuGHicqzNOVLyPfxy1W7cMmmFpfNu6VU+7J8YpbyzZz5dnoOk2lUs/Yz0HGtHMmfsPYYHvigZkoGmdUMrr9acQKE2xeiZL+XcxSJMSMvC5MU7cPcVTf1ut94rW4SZ8fWaPbi+0yWoUamC5j56B9Q98f3GoBcAPXKPnkWrBtVD3v9iYenPK5RRwUZ/Ty/M3ILbuyWhYvk4nC8swtvzt+Ph/imoWjH8lwIzzX3Hzmg3wSSPmx36QUP0YYBJIU+dL0RRMaNmZd/va8beY/jb1xtK3uccPoOcw2fw8s+/Y/qDV9hSVn+kRqBYq3GR9s7MUXdi3fbhSrwxd5vpzzUzm+J3Xh3duy2e8TGQWyat0JV/raXTS7+WzKjqLz/71LlCj5x+ZmD9nmN49ofNeMZr/h81PTOMMge/CwxFwcnzGPe/TEM1yVmZpdOHGB0VXFgUWm3ic6Up59u1e/Hh4p14d2Hg0bn++GvTD5W/JhotnV8Ovb/Fau5xJWru8N71lfno9JLv9DFuelOd91rcv+NNAkEQ7hvc7IJTmLGh9OKxZtcRfLDI2PTQWn83N30QfLSw2kEbqtaTFvtePJldU3Sr+zTU0ncfxTydA4a8qdMBi/zUIDJyj3m8zz16tqQpIlo75B7+ZgOmrd2LOZv0T0/8yk/hz0931wAvFrl+9lY0kVkhUuN37OQ+p9RX0zzGKhl1s81NRtI0pJNdk3l5zziYnnMEqcl1/G7vb94fvbQ6ZLU64+ZuORBS563eKXyDMXuegbCF/RLZ+adw5PQFDJ+yqmQZGRjre8HExcH1WZFjtkLg7+dk9PcTieYgow6dOo9jZy4ioXrFoNteLCrGsTMXUKtKvEefyu7DZ9A+saYt5XNcjSCSUybr+cO5ZfJK3PXxquAbRgE7L9Za3DnkK3YcxoQ0fZ3SdluaVeDTqa++kBUWscd3zsqpAt6evx0/ZpiftV3v4PJPl++ydM78bQeiZzBnNNmw51jYm74cVyMItR0914I2Or0DXvQMLItVZqq3n6jm6J+QloVHBrYqeT94whIMad9I10UtnLNqXDPBc/TqSNWIWbOMNk16874v6fVG4GkpXvrpd7RrVAOXN6+rub9Rr2pMm8FsTTZXWRDOcUaOqxFk+snnDuaFmaHNjq0eQGPkwRN67hyNdK5FC3c6nxUmqS6E2w6cxDtp23VNrRDu+e3VtJISIu2zFTnIOngSe48E/9n9b31oo7D1yso/hQtFkR9oaYevVu/GRgMDOJ8LY43bcYHAzsfBLdlegFFT15R8Rnb+SbR9YW5IUxh8vy74Pj9ssD7zJdzM3FX+O8SsrZU7ratxaX2dNuWeCNtANas84WfErrfp6blIHjcbv+edsGVKhEIdQeApC28mwmlCWhaG6ZhK3s2OZ4v447hAYLXkcbNLniT2wBfpWLy9oCRTYKuSGqlnPhhv/iYsi0XuaYa1FJiYwTNaLcsuCL5RFDCTxaJ3Dh47GJkfLFIGvr04+EYGLc0qsK02a9UTygYT0R9ElE1E4zTW/4OIMpR/m4moiIjqKOtyiGiTsi7d9+jRb6ufXHgzrLxrjbQn/5fp96ITzrEPwpOZQPDx0l3YbMPMvR8uMdfvYZSVfTZqRp+foMe0tXt9+pysYjoQEFEcgIkAhgBoB+AOImqn3oaZ32TmzszcGcDTABYz8xHVJv2U9almyxNMOFofn/w+E0u2x8ZdYbjYdSczadEOn6dERVoUP+KhxL5jZwM+kW9F9iG8OW+b30FrB06cK5ki20ozM/LC+gNcmhW5mk00sSJrqDuAbGbeCQBENA3AMAD+RsrcAeAbCz43JOoJqewya2MeZm3Mw3t3XOZaEAMXBrt9sCgbDw9Isfy4ofYT2MnMaPFwGfHxauSpsti8H0d658euO+UWCdXCWi4RGVYEgkQA6ka7XACXa21IRFUADIbrgfVuDOBXImIAHzLzFD/7jgYwGgCaNGkScmHD2cHq7kybvWk/ngzh4dxlyZxNB3TPIhlLtDpM88M8sZpR6jRcN38ZaI9Nt36K7mCsGpQo9LMiEGjlDvi7B74ewHKvZqGezJxHRPUBzCeibczs0xCmBIgpAJCamhpz99h93lwU6SIIG2zdfxIV4iI5vrfs+SpAk5WwhxWdxbkAklTvGwPwN9xxOLyahZg5T/k/H8APcDU1xZQ1uw7jxLmLPnOl7Mh3di3ACTL2HovKsQFCGGFFjWAtgBQiagZgH1wX+zu9NyKimgD6ABihWlYVQDlmPqm8vhrAyxaUKawm/rZD80lf70TJNAhCCBGI6UDAzIVENBbAPABxAKYy8xYiekhZP1nZ9EYAvzKz+ja5AYAflOlsywP4mpnnmi1TJPibTlkIIaKdJXMNMfMcAHO8lk32ev8ZgM+8lu0E0MmKMgghhAiNjCwWQggb2DU9tr+ns5khgcAiJ0J8WpcQQhixy4Z5rCQQWORiGZ0xUQhR9kkgEEIIh5NAIIQQDieBQAghHE4CgRBCxBA7eiMlEAghRAyxY5ZuCQRCCOFwEgiEECKmWF8lkEAghBAOJ4FACCEcTgKBEEI4nAQCIYRwOAkEQgjhcBIIhBDC4SwJBEQ0mIj+IKJsIhqnsb4vER0nogzl3wt69xVCCFHKjgFlpp9QRkRxACYCGATXg+zXEtEsZv7da9OlzHxdiPsKIYQAcKGw2PJjWlEj6A4gm5l3MvMFANMADAvDvkII4TjT1u61/JhWBIJEAOqS5SrLvF1BRBuJ6BciutTgviCi0USUTkTpBQUFFhRbCCFiT2FxdNYISGOZdyvWegBNmbkTgPcA/GhgX9dC5inMnMrMqQkJCaGWVQghhBcrAkEugCTV+8YA8tQbMPMJZj6lvJ4DoAIR1dOzrxBCCHtZEQjWAkghomZEFA9gOIBZ6g2IqCERkfK6u/K5h/XsK4QQwl6ms4aYuZCIxgKYByAOwFRm3kJEDynrJwO4BcBfiKgQwFkAw5mZAWjua7ZMQghRVkVl+ihQ0twzx2vZZNXr9wG8r3dfIYQQ4SMji4UQIobIE8pMyjl0OtJFEEIIU1geTGOOHfm3QggR6xwVCIQQQvhyVCCwo21NCCFinaMCgRBCCF8SCIQQIoZI1pAQQgjLOSoQSBeBEEL4clQgEEKIWGfHDa0EAiGEiCHSR2CSpI8KIYQvRwUCIYQQviQQCCGEw0kgEEKImBKlk84R0WAi+oOIsolonMb6u4goU/m3gog6qdblENEmIsogonQryuOPHbP2CSFEOG3df9LyY5p+MA0RxQGYCGAQXM8gXktEs5j5d9VmuwD0YeajRDQEwBQAl6vW92PmQ2bLIoQQZd2+Y2ctP6YVNYLuALKZeSczXwAwDcAw9QbMvIKZjypvV8H1kHohhBBRwIpAkAhgr+p9rrLMn/sA/KJ6zwB+JaJ1RDTa305ENJqI0okovaCgwFSBhRBClLLimcWksUyzMZ6I+sEVCK5SLe7JzHlEVB/AfCLaxsxLfA7IPAWuJiWkpqZKY78QQljEihpBLoAk1fvGAPK8NyKijgA+BjCMmQ+7lzNznvJ/PoAf4GpqsoUMKBNCxLq4clr33uZYEQjWAkghomZEFA9gOIBZ6g2IqAmAGQBGMvN21fKqRFTd/RrA1QA2W1AmTcUSCYQQwofppiFmLiSisQDmAYgDMJWZtxDRQ8r6yQBeAFAXwAdEBACFzJwKoAGAH5Rl5QF8zcxzzZZJCCHKKuvrA9b0EYCZ5wCY47Vssur1/QDu19hvJ4BO3suFEEJoKyyO0gFlsUJahoQQwpejAoEQQghfEgiEEMLhJBAIIYTDOSoQSB+BEEL4clYgkNlHhRDCh6MCgRBCCF+OCgTSNCSEEL4cFQiEEEL4clQgkAqBEEL4clQgEEII4ctRgYClk0AIIXw4KhAcPHEu0kUQQoio46hA8NCX6yNdBCGEiDqOCgRCCCF8SSAQQgiHsyQQENFgIvqDiLKJaJzGeiKid5X1mUTURe++Qggh7GU6EBBRHICJAIYAaAfgDiJq57XZEAApyr/RACYZ2FcIIYSNrKgRdAeQzcw7mfkCgGkAhnltMwzAF+yyCkAtImqkc18hhBA2siIQJALYq3qfqyzTs42efQEARDSaiNKJKL2goMB0oYUQQrhYEQhIY5n3yC1/2+jZ17WQeQozpzJzakJCgsEiCiGE8Ke8BcfIBZCket8YQJ7ObeJ17CuEEMJGVtQI1gJIIaJmRBQPYDiAWV7bzAJwt5I91APAcWber3NfIYQQNjJdI2DmQiIaC2AegDgAU5l5CxE9pKyfDGAOgKEAsgGcAXBvoH3NlkkIIYR+VjQNgZnnwHWxVy+brHrNAMbo3dcuLw+7FC/MlDgjhBBqjhpZXKl8XKSLIIQQUcdRgSCxduVIF0EIIaKOowJB84SqkS6CEEJEHUcFAiGEEL4kEAghhMM5KhDIkyqFEMKXowKBEEIIXxIIhBDC4SQQCCGEw0kgEEIIh5NAIIQQDieBQAghHM5RgUCyR4UQwpejAoEQQghfEgiEECKGVKpg/WVbAoEQQsQQO2ZIMBUIiKgOEc0noizl/9oa2yQR0W9EtJWIthDR31XrXiSifUSUofwbaqY8wbDMMSGEED7M1gjGAVjAzCkAFijvvRUCeJyZ2wLoAWAMEbVTrX+HmTsr/8LypDIhhBClzAaCYQA+V15/DuAG7w2YeT8zr1denwSwFUCiyc8VQghHirqmIQANmHk/4LrgA6gfaGMiSgZwGYDVqsVjiSiTiKZqNS2p9h1NROlElF5QUBBSYaVlSAghfAUNBESURkSbNf4NM/JBRFQNwP8APMLMJ5TFkwC0ANAZwH4Ab/nbn5mnMHMqM6cmJCQY+WghhCgz2IYRUeWDfijzQH/riOggETVi5v1E1AhAvp/tKsAVBL5i5hmqYx9UbfMRgJ+NFF4IIZymOAqbhmYBGKW8HgVgpvcGREQAPgGwlZnf9lrXSPX2RgCbTZZHCCHKNDuyH80GgvEABhFRFoBBynsQ0SVE5M4A6glgJID+GmmibxDRJiLKBNAPwKMmyyOEEGWaHTWCoE1DgTDzYQADNJbnARiqvF4GgPzsP9LM5wshhDBPRhYLIUQMGdb5EsuPKYFACCFiSMOalSw/pqMCQc0qFSJdBCGEMCcKs4ZiSo1KEgiEELGtOAqzhoQQQoRR49pVLD+mBAIhhIghV6XUs/yYEgiEECKGVK4QZ/kxJRAIIYTDSSAQQgiHk0AghBAxhDTnaTBHAoEQQjicBAIhhHA4CQRCCOFwEgiEEMLhJBAIIUQMibqH1xNRHSKaT0RZyv+aD58nohzlATQZRJRudH8hhBD2MVsjGAdgATOnAFigvPenHzN3ZubUEPcXQghhA7OBYBiAz5XXnwO4Icz7CyGEMMlsIGjAzPsBQPm/vp/tGMCvRLSOiEaHsL8QQgibBH1mMRGlAWiosepZA5/Tk5nziKg+gPlEtI2ZlxjYH0oAGQ0ATZo0MbKrEEKUGXaMLA4aCJh5oL91RHSQiBox834iagQg388x8pT/84noBwDdASwBoGt/Zd8pAKYAQGpqqg395kII4Uxmm4ZmARilvB4FYKb3BkRUlYiqu18DuBrAZr37CyGEsJfZQDAewCAiygIwSHkPIrqEiOYo2zQAsIyINgJYA2A2M88NtL8QQojwCdo0FAgzHwYwQGN5HoChyuudADoZ2V8IIUT4yMhiIYRwOAkENnn/zssiXQQhhNBFAoFNrut4SaSLIIQQukggEEIIh5NAIIQQDieBQAghYgjB+qHFEgiEEMLhJBAIIYTDSSAQQgiHk0AghBAOJ4HApIFt5REK0eDBPs0jXQQhYpYEApNqVKoQ6SIIAH/qJAP4hAiVBAKT/tqvZcnrmpVdQaFSBdePtWX9ahEpkxNVLC9fZSFCJX89JqkvQKuelolUhRCxRwKBEELEkFpVrG+OlkBgI2Z5oqaIDTP+emWki1Cm9GxZ17ZjV6oQZ/kxTQUCIqpDRPOJKEv5v7bGNq2JKEP17wQRPaKse5GI9qnWDTVTnkiw45ciAuuQWDPSRfArZ/y1kS5CSLo08fnTFSZMGZka6SIYYrZGMA7AAmZOAbBAee+Bmf9g5s7M3BlAVwBnAPyg2uQd93pmnuO9f7SrUzW+5DVDagB2+teNHQAA7TUCQVw5qdxaLV464ENWJT62bhDN/qaHAfhcef05gBuCbD8AwA5m3m3yc6NGXDnfCaDsmBTKKrH8x317tyT845rWePbatj7rmtWrGoESlW3uLDgRmlduaG9q/1FXNLWoJMGZvSo0YOb9AKD8H2x01XAA33gtG0tEmUQ0VatpyY2IRhNROhGlFxQUmCu1Rf5xTetIF8FR4soRxvRriWoVTT1q2/EGtWsQ6SJYQp2x17Fx9DUXjuxh7kI+MpoCARGlEdFmjX/DjHwQEcUD+BOA71SLJwFoAaAzgP0A3vK3PzNPYeZUZk5NSEgw8tEePrrbura7To1rWXascOnSpFakiwAA+GvfFpEugmNVtbDZonlCaDWxh/pY+/uvVD46m2KiuT9LLWggYOaBzNxe499MAAeJqBEAKP/nBzjUEADrmfmg6tgHmbmImYsBfASgu7nTCe7SS2qEvO+VLUozAWb89UpclVLPiiKF1cejukW6CAC0m9REeFS1sEb14YiuSHust2XHi2WVNRJHZo3tiXFD2kSgNMaYbRqaBWCU8noUgJkBtr0DXs1C7iCiuBHAZpPlsZX6DyipdpWS15//uTt+HNMz5ON+ek/4Ls5GmlXcI6TtkFC9om3HFoGVI31BWM9W1SqVR8v61XV/dt2q8biqZT3UqxYffOMg/KVmRFNHLRFZXvuxg9m/9PEABhFRFoBBynsQ0SVEVJIBRERVlPUzvPZ/g4g2EVEmgH4AHjVZHlup/zAqq75sfVoloHNSLZ/t9eYQee87oE10TGS39eXBkS5CTOjXOvSmSr1uT02y/TPCoUfzuvjy/sv9BiNDTSmqP7BLE0tr+tE4/1dfi74j00b3sOQ43kwFAmY+zMwDmDlF+f+IsjyPmYeqtjvDzHWZ+bjX/iOZuQMzd2TmP7k7nu1U3kSTRPdmdQAAb9/WydYOS6PtrnZ0lN3StTFI551jKMrSWLsPbcoZb1LHVescN6QN7urRxGPd0A4NbflMt9YN9N/l61W3anzQu2M9f56PD2oFAHjz1o4ly54ZWppJZuPXNmT/GX5ZyPs2rl255HWP5vYMVIvdXMIQ1a9RKeD6/7u1k98Uy76tE7DxhatxU5fGhj5z8oguupuOfv7bVagQp//XolUTsUJdC6ruarMfvgrPDm2LJ65uZelx1V68vp2u7RY+3seyzxzZo2nAlNyp94QeJFqobgguqVXZY53ZdNmf/3YVavuZqqBny7qY92hvyy+o654fhA4W3LSM7d8Svz7aG8M6J5bcBFWIK+f3fKKBmVTccEyo6LhAEEyLhKp4/jr/F5SaAb5s7vED3l/IlvWrB7xgq+cOaZ9YE2NUM5oG880D5quKP47piZVP99dc18mi2sall9TEA72bY2z/FNtG397Tsxlu6Ro8SDdPqIZXTeZ4uwW6WGrV1J4arL/jUF0jq1fNt0/FaN9S07ql/VrtE2vitm6u5qaHB6R4bPfV/cG/U0Pau2ok7g7Sjf+82lBZ/NHz8yEitFJqLNNG9/D57tpR2/z8z7bnsUSUBAINyao/GLVgX7DK8XF47cb2+PbBKwx9nncTjN6sjjdu7ujRV6HXY4M878o7J9VCo5qed5xXtohsRlSonYkpOqf+HtGjKZ4Zak82x9u3dcLdVzTFTI1a4AO9muk+TqAbcmagX5v6eP9O/U0O912l/dla2S5ahncr7ad45/bOmP9ob9Sq4vo9WTX47MqW9fCtgXbwKvHlfb67ehmpTV/VMvYyBI1wZCAY3Tvw06x6pSTgL6o898Ra+r9od13eFEl1tAOJEYue6Is2DQO30zYLMYf7mksDty9ve2Uw+rRSOrci1OB6R/cmwTcCcEPnSyy7u7fKTV0a4+Vh7TX7WMobaPbT86O/ruMllnVEevNujnpM1axXqUIcUmzoRwA8p22pEEd49Yb2+HNPAwFU51f22g6NAq438nevNv/R3rb2r9nBkYHgMh13Ak8NboMfx/TE1HtSQ7rrNiu5XtWIpcGZmUjP3aHulvZYH0y4vbPJEvk3YfhlGBHiCE4jTQiRTHcNdknRe8kZ2qERmidU9Vsz8PbR3al4947SGkf96pVs76QGPLPtsl4bihE9muIFnf0/APDIwBSfZUZqYm7u3/nHBgehJtYOLYBEkiMDgV6dk2qhfxvrhuNfbXBovxU1Cy1Gbla8Nw2WKnnTZYke71vWr4YbvJbFoqmjumFY53A/DtPzp99flVasFfyCpR3Xq1YRCx/vi2Slo7mrMuOov5TNetUq+jwC9IO7ukbtDKupya6bEK2EkGev1R9I3NxJAHqbavsrzy8PNlhSz7gCPX1dVpJAYCevO85A2SX1qlXEowM92+7jDTQj2MU7aHh/yW82mEFlN+/mDKt0aFwzpBTAVhrNJw/390wG8NcE0bCmZy1EfXeudZ63phr7XVx9aUOsfXZg0BHyCx/vYyr7Sa9WDcw92vXd4Zdh7iO9UN2i1O43b+mIv/Ztgcub1dH1bJG3b+uE5eP6o6IF0114J5zMHNMT790RegpqMJG/0jiA98X0lRvaI/25gR7L0p8biL9rVGkDaRgkFVYP73lnenldFALd29zRvQneuq2Tx7L6NSI7Yvi6jo1Qo5K+C4EVySXqwUtaGVaNa1fBnId7eSxT37GueXYA5j2qPUVDlXjP8/A3dsXdHh1Ktoy6ySvjhUHIfNE3+6d5QrWQa8bNDaS5Xt/RXI2rcnwc2jSsYVm3Vv3qlfDk4DYoV44QV4789i0O75aEOQ/3QsXycSH3KwBA+0T/0990SqqF6zvZVyOVaRy9WDkPi7enh7bFuYvFuLlLos8fuZZgf9ehNh2p/07UnVq7Xtf3XKCP707FI99m4AWNNNv+bRrgiz93x4ET5wL2xbjvsO6+oim+WOk7K7m6jO/c3gm7Ck7rKhsRoVdKAmZv0j828Z4rk1G7SjzeSdsedNtXhl2K52duwdAODdE9uQ6GK53agX527QLMb1W/umcwjytHKCp2/WwqxLl+CkY6mM1wZwBZJWf8tfh27R489b9NQbcdf1MH3GxTc8gCC8aNEBGeGdoWTw9pg2ZPlz42ZcW4/pbUQsff1AGD2zfEB4t2YMqSnaaPZ5TjawTqIdsfjuyqWZW3SmKtyvh4VKquIAAEvsO75tLSO7TbDE4/0NJPiiUR+WQ7aGU/DGzXAJtfusZvJ3rvVgm4LTVJV1aJnpu3Gy9rjMeu1j/l9591doa6VSxfDnWquu7sbw1yMaqpXCzLEeGens1KOta1fnZGuPuPVo4rzYkf068lHuzTHHddri+DKhozVYLdWHVtWhuzxvbE8O5NSgZSmh8H4PlzaJHg+r6vfmZAybKv7r88tCN7/YyNBAH3eCGtZw4P794EtarEl0xdfUvX8E4p4shA0LtVaYenesh2sLTKSM6K8OYtHT3y0ife2aXk9YgeTZEz/tqSO0h/Xhl2KQDPL/N7BvLQY0XrIGm3bloXnIpBJtpz13JuNNEB7v7Y1Kalj994787LsPZZz+bCKvHl8fSQtkGzuNyBvX6NipiuGsOiNY4hXNzjCoa2D5yi2SGxJjqGaTr3BqomuZ4t62Fox0YezW339kzGc9faN1WFOyW7UoC+wqQ6VZAz/lq0blgdjWtXQdX4ODxpYBBiqBwZCIw2/0TqPks9ZfatqUnopGpq0WouWPSPfh4XArV2jWpg5BXJPsuDPVPByLnrvXM1IpRpw6tVLO8RKIMKcpJ1VXnt7j/UAW3NZ5OpA1bF8nEhp6j+45rW+PK+y9GlSW2P9N1OSbWw5aVrTJdTL3UegbumXc7E3F5mO4+DSaxVGZtVPx9m4P5ezbHoib745/XtLH8euTsTaEDbBpg8omvQ6akrVYjDlpcHB71BtYIjA0GsuLdnss+yp4e08TvcPbFWZZ88fqu1beT/wvya8kxhq8wc0xNf65juQMu1HRth7iO9PO7wvLnvWmtV9mwbv6mL591+tE+ZXSGunN/MH/dNj3darx3UGWWNI5hLb/RO3n0D456QMrleVdxrYACbXr1SEpAz/lok1amCwe0begycizTpLIYrc0PvHO3hpNXm+6CBuc2Hd0tCYTHj+3W5IZehTaPqSN99FJ0a18RLw9qjvcYdeuekWsjYeyzkz3Bb8HgfvDF3G+ZtOQgQedSAQtGmYQ20aVgDr87eqrn+9m5JKGbG7d2S8MvmAwCAulUr4syFMx7bxfr0ApHO+//6gctRcPK86eP8977ulhzHW3Ld0Ebo92hu7qbLnebqr88unCQQwDdzwyrucQP+QkxSncpBc477tEoImHUSyPibO2LzvuOagaBWlQo4duZi0GO475YHtG3gd26Wr+6/HEdOXwipjADw75s7oFrFCmiRUA1tG9VwBQIL/TimJ26YuNxneVw5KhmYdX3HRjh/sQjDOidi3IxMAK5n+3ZMrOkx3YgwTmvequpKiq9Wx2kDJQXZOwmiV4q+qTTCdUv3l776J4fUMrh9Q7x+UwdT/U1WkUBgo4/uTsV36Xv9Thm89EntGT/VzM562LphdQxsWx+PDtKe/jno9AU6/qqqVixvKu329m7W9y2o6ZlcjIhwq3Lh6dG8Lmas34ex/VqarpU4yY2XJWJ6ei66N6uDqkEy427u0hjnCos1H7hTq0o8dv5rqCWdtT1b2jN/P2A+4BCR7jm17GYqEBDRrQBeBNAWQHdmTvez3WAA/wEQB+BjZnY/yawOgG8BJAPIAXAbMx81U6ZoklSniqG0RztUiCtn6jnF7iYzJz1i+NaujdG3dYJtNcVA3EkAsfLQc7V/3dgBz17bTtdMpOXKUUmqpL/1oXI3qXZqXFPXlNrCfGfxZgA3AVjibwMiigMwEa6H17cDcAcRuUcijQOwgJlTACxQ3kedF65vh6Q6lUueGFUW9Faq2cHSJR/o3RwjejSxvPPMncL7p87aHbMJQaahnjKya0k6rBF6MpuIKCJBAHDNvPmf4Z0xNYzPsbZK+bhylk1HHU7uMteOos7bcDNVI2DmrUDQgSzdAWQz805l22kAhgH4Xfm/r7Ld5wAWAXjKTJn0+v3la3D4lL527V4pCbqacaLF3Ed6Bf2DfPPWjnj86lZBB7dVq1ger95gbTYQ4Jq2QKsT845uTVC9UgVcF2SK4KtDSKm7PTXJ8swmvcbf1AEtlE7BCsrdrr8n0Q3r7L/NOLFWZew7dtb6ApYhl15SA9dc2kCzOXTNMwNw8nyhx7JbujZGEbPhid6q65zKJBaQnsmUgh6EaBGAJ7SahojoFgCDmfl+5f1IAJcz81giOsbMtVTbHmXm2t7HUNaNBjAaAJo0adJ1927faQmE8OfcxSLEx5Uz1eRglQuFxXjr1z8wtn9LVDf4oPX8k+eQnX8q4IODlmYVoHqlCrY9xtTpsvNP4Zs1ewI+yTBaEdE6ZvaZQTBoSCOiNABat1/PMvNMPZ+tscxw9GHmKQCmAEBqamoZevS5CAerBweZEV++HJ4e6n98QyD1q1cK2mylN7tGhKZl/WoxGQQCCRoImHlgsG2CyAWgTg1oDCBPeX2QiBox834iagQg3+RnCSGEMCgcI4vXAkghomZEFA9gOIBZyrpZAEYpr0cB0FPDEEIIYSFTgYCIbiSiXABXAJhNRPOU5ZcQ0RwAYOZCAGMBzAOwFcB0Zt6iHGI8gEFElAVgkPJeCCFEGFnSWRxuqampnJ6uOWRBCCGEH/46i2XSOSGEcDgJBEII4XASCIQQwuEkEAghhMPFZGcxERUACHVocT0AhywsTiyQc3YGOWdnMHPOTZnZZ8RhTAYCM4goXavXvCyTc3YGOWdnsOOcpWlICCEcTgKBEEI4nBMDwZRIFyAC5JydQc7ZGSw/Z8f1EQghhPDkxBqBEEIIFQkEQgjhcGU2EBDRYCL6g4iyicjnWcjk8q6yPpOIukSinFbScc53KeeaSUQriKhTJMpppWDnrNquGxEVKU/Mi1l6zpeI+hJRBhFtIaLF4S6j1XR8r2sS0U9EtFE553sjUU4rEdFUIsonos1+1lt7/WLmMvcPQByAHQCaA4gHsBFAO69thgL4Ba4nqPUAsDrS5Q7DOV8JoLbyeogTzlm13UIAcwDcEuly2/w7rgXX88CbKO/rR7rcYTjnZwD8W3mdAOAIgPhIl93kefcG0AXAZj/rLb1+ldUaQXcA2cy8k5kvAJgGYJjXNsMAfMEuqwDUUp6SFquCnjMzr2Dmo8rbVXA9LS6W6fk9A8DfAPwPsf8EPD3neyeAGcy8BwCY2QnnzACqExEBqAZXIChEDGPmJXCdhz+WXr/KaiBIBLBX9T5XWWZ0m1hi9Hzug+uOIpYFPWciSgRwI4DJYSyXXfT8jlsBqE1Ei4hoHRHdHbbS2UPPOb8PoC1cj8DdBODvzFwcnuJFjKXXr6DPLI5RpLHMO09WzzaxRPf5EFE/uALBVbaWyH56znkCgKeYuch1wxjT9JxveQBdAQwAUBnASiJaxczb7S6cTfSc8zUAMgD0B9ACwHwiWsrMJ2wuWyRZev0qq4EgF0CS6n1juO4WjG4TS3SdDxF1BPAxgCHMfDhMZbOLnnNOBTBNCQL1AAwlokJm/jEsJbSW3u/1IWY+DeA0ES0B0AlArAYCPed8L4Dx7Go8zyaiXQDaAFgTniJGhKXXr7LaNLQWQAoRNSOieADDAczy2mYWgLuV3vceAI4z8/5wF9RCQc+ZiJoAmAFgZAzfIaoFPWdmbsbMycycDOB7AH+N0SAA6PtezwTQi4jKE1EVAJfD9azwWKXnnPfAVQMCETUA0BrAzrCWMvwsvX6VyRoBMxcS0VgA8+DKOpjKzFuI6CFl/WS4MkiGAsgGcAauu4qYpfOcXwBQF8AHyh1yIcfwzI06z7nM0HO+zLyViOYCyARQDOBjZtZMQYwFOn/HrwD4jIg2wdVk8hQzx/TU1ET0DYC+AOoRUS6AfwKoANhz/ZIpJoQQwuHKatOQEEIInSQQCCGEw0kgEEIIh5NAIIQQDieBQAghHE4CgRBCOJwEAiGEcLj/BxDheuI4X/ImAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time = np.linspace(0, 1, 44100)\n",
    "plt.plot(time, wave.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d043e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIkernel",
   "language": "python",
   "name": "aip2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
